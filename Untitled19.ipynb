{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dce2c27c-03f9-4cc3-802d-e7e6ce916cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   query  \\\n",
      "0     \"How do I sign up for an account?\"   \n",
      "1       \"How do I log in to my account?\"   \n",
      "2          \"How do I reset my password?\"   \n",
      "3          \"How do I delete my account?\"   \n",
      "4  \"How do I change my account details?\"   \n",
      "\n",
      "                                            response  \n",
      "0  \"To sign up, click the 'Sign Up' button on the...  \n",
      "1  \"To log in, click the 'Login' button, enter yo...  \n",
      "2  \"To reset your password, go to the login page ...  \n",
      "3  \"To delete your account, navigate to your acco...  \n",
      "4  \"Go to your profile settings and click on 'Edi...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "dataset = pd.read_csv(\"Chatbot.csv\")\n",
    "\n",
    "# Check the dataset structure\n",
    "print(dataset.head())\n",
    "\n",
    "# Ensure the dataset has clear columns 'query' and 'response'\n",
    "if \"query\" not in dataset.columns or \"response\" not in dataset.columns:\n",
    "    raise ValueError(\"The dataset must contain 'query' and 'response' columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f142ed9a-521c-4eac-8fee-d5e2d6a04f5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd5bd122a8b40f191f613ae9568bdf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '\"How do I sign up for an account?\"', 'response': '\"To sign up, click the \\'Sign Up\\' button on the homepage, fill out the required information, and confirm your email.\"', 'text': '<|query|> \"How do I sign up for an account?\" <|response|> \"To sign up, click the \\'Sign Up\\' button on the homepage, fill out the required information, and confirm your email.\"'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Prepare the dataset by concatenating query and response with markers\n",
    "def add_markers(examples):\n",
    "    examples['text'] = f\"<|query|> {examples['query']} <|response|> {examples['response']}\"\n",
    "    return examples\n",
    "\n",
    "# Convert pandas DataFrame to HuggingFace Dataset\n",
    "dataset = Dataset.from_pandas(dataset)\n",
    "dataset = dataset.map(add_markers)\n",
    "\n",
    "# Print example with markers\n",
    "print(dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb175bf3-f44c-4461-a917-a066f0eb6e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74e1f33ec5344fa1b5b4adeca9b5350f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "626790b7828f4c538901a3e3135374a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 09:07, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.322200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.454100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.296000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.239300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=40, training_loss=0.8278794407844543, metrics={'train_runtime': 567.3947, 'train_samples_per_second': 0.67, 'train_steps_per_second': 0.07, 'total_flos': 24822743040000.0, 'train_loss': 0.8278794407844543, 'epoch': 20.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "\n",
    "# Initialize the GPT-2 tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set EOS token as pad token\n",
    "\n",
    "# Tokenize function to handle input formatting\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set input_ids as labels for language modeling task\n",
    "def prepare_data_for_gpt2(examples):\n",
    "    import torch\n",
    "\n",
    "    # Convert input_ids to tensors if they are not already\n",
    "    input_ids = torch.tensor(examples['input_ids']) if not isinstance(examples['input_ids'], torch.Tensor) else examples['input_ids']\n",
    "    \n",
    "    # Clone input_ids to create labels\n",
    "    examples['labels'] = input_ids.clone()\n",
    "    return examples\n",
    "\n",
    "\n",
    "# Apply the preparation\n",
    "final_dataset = tokenized_dataset.map(prepare_data_for_gpt2)\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=12,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=final_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b271538-0326-4e0c-85fd-d026c56fb23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot response: How can i search courses?\n",
      "\n",
      "To find out more courses, go to the courses page and click the 'Search'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    input_text = \"How can i search courses?\"\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate response from the fine-tuned model\n",
    "    output = model.generate(\n",
    "        inputs['input_ids'], \n",
    "        attention_mask=inputs['attention_mask'], \n",
    "        max_length=150,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        do_sample= True,\n",
    "        temperature=0.7,  # Adjust for more creativity vs. determinism\n",
    "        top_k=50,         # Consider top k most likely next words\n",
    "        top_p=0.9,  \n",
    "        no_repeat_ngram_size=3\n",
    "    )\n",
    "\n",
    "    # Decode and print the response\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "   \n",
    "    print(f\"Chatbot response: {response}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c78ca2f5-870c-46e5-b416-90efb02e94a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Desktop\\test\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697a9f47-891f-49f8-9e8d-782d064ab390",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
